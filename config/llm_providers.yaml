# LLM Provider Configuration
# This file defines the available LLM providers and their models for AI agents

# Default provider to use when none is specified
default_provider: anthropic

# Default model for each provider (used when only provider is specified)
default_models:
  anthropic: claude-sonnet-4-20250514
  openai: gpt-4o
  google: gemini-2.0-flash
  local: llama3.2:latest

# Provider configurations
providers:
  anthropic:
    enabled: true
    api_base_url: https://api.anthropic.com
    api_version: "2023-06-01"
    # API key loaded from environment: ANTHROPIC_API_KEY
    models:
      - id: claude-sonnet-4-20250514
        name: Claude Sonnet 4
        max_tokens: 8192
        context_window: 200000
        supports_tools: true
        supports_vision: true
      - id: claude-opus-4-20250514
        name: Claude Opus 4
        max_tokens: 8192
        context_window: 200000
        supports_tools: true
        supports_vision: true
      - id: claude-3-5-haiku-20241022
        name: Claude 3.5 Haiku
        max_tokens: 8192
        context_window: 200000
        supports_tools: true
        supports_vision: true
    rate_limits:
      requests_per_minute: 60
      tokens_per_minute: 100000

  openai:
    enabled: true
    api_base_url: https://api.openai.com/v1
    # API key loaded from environment: OPENAI_API_KEY
    models:
      - id: gpt-4o
        name: GPT-4o
        max_tokens: 16384
        context_window: 128000
        supports_tools: true
        supports_vision: true
      - id: gpt-4o-mini
        name: GPT-4o Mini
        max_tokens: 16384
        context_window: 128000
        supports_tools: true
        supports_vision: true
      - id: gpt-4-turbo
        name: GPT-4 Turbo
        max_tokens: 4096
        context_window: 128000
        supports_tools: true
        supports_vision: true
      - id: o1
        name: OpenAI o1
        max_tokens: 100000
        context_window: 200000
        supports_tools: false
        supports_vision: true
      - id: o1-mini
        name: OpenAI o1 Mini
        max_tokens: 65536
        context_window: 128000
        supports_tools: false
        supports_vision: false
    rate_limits:
      requests_per_minute: 500
      tokens_per_minute: 150000

  google:
    enabled: true
    api_base_url: https://generativelanguage.googleapis.com/v1beta
    # API key loaded from environment: GOOGLE_API_KEY or GEMINI_API_KEY
    models:
      - id: gemini-2.0-flash
        name: Gemini 2.0 Flash
        max_tokens: 8192
        context_window: 1000000
        supports_tools: true
        supports_vision: true
      - id: gemini-1.5-pro
        name: Gemini 1.5 Pro
        max_tokens: 8192
        context_window: 2000000
        supports_tools: true
        supports_vision: true
      - id: gemini-1.5-flash
        name: Gemini 1.5 Flash
        max_tokens: 8192
        context_window: 1000000
        supports_tools: true
        supports_vision: true
    rate_limits:
      requests_per_minute: 60
      tokens_per_minute: 1000000

  local:
    enabled: true
    # Local providers support Ollama and LM Studio compatible APIs
    api_base_url: http://localhost:11434  # Default Ollama endpoint
    # Alternative: http://localhost:1234/v1 for LM Studio
    models:
      - id: llama3.2:latest
        name: Llama 3.2
        max_tokens: 4096
        context_window: 128000
        supports_tools: true
        supports_vision: false
      - id: mistral:latest
        name: Mistral
        max_tokens: 4096
        context_window: 32000
        supports_tools: true
        supports_vision: false
      - id: codellama:latest
        name: Code Llama
        max_tokens: 4096
        context_window: 16000
        supports_tools: false
        supports_vision: false
      - id: llava:latest
        name: LLaVA
        max_tokens: 4096
        context_window: 4096
        supports_tools: false
        supports_vision: true
    rate_limits:
      requests_per_minute: 100
      tokens_per_minute: 500000

# Agent-specific LLM configurations
# Override default provider/model for specific agents
agent_configs:
  orchestrator:
    provider: anthropic
    model: claude-sonnet-4-20250514
    temperature: 0.3
    max_tokens: 4096
    system_prompt: |
      You are an orchestrator agent responsible for breaking down tasks into
      actionable work items and coordinating the workflow between specialized agents.

  dev_worker:
    provider: anthropic
    model: claude-sonnet-4-20250514
    temperature: 0.2
    max_tokens: 8192
    system_prompt: |
      You are a development agent specialized in writing, reviewing, and
      refactoring code. Focus on clean, maintainable solutions.

  test_worker:
    provider: openai
    model: gpt-4o-mini
    temperature: 0.1
    max_tokens: 4096
    system_prompt: |
      You are a testing agent specialized in creating comprehensive test cases,
      identifying edge cases, and ensuring code quality.

  requirements_manager_worker:
    provider: anthropic
    model: claude-sonnet-4-20250514
    temperature: 0.4
    max_tokens: 4096
    system_prompt: |
      You are a requirements management agent specialized in analyzing,
      clarifying, and organizing project requirements.

  code_review_worker:
    provider: anthropic
    model: claude-sonnet-4-20250514
    temperature: 0.2
    max_tokens: 4096
    system_prompt: |
      You are a code review agent specialized in identifying issues,
      suggesting improvements, and ensuring best practices.

  documentation_worker:
    provider: google
    model: gemini-2.0-flash
    temperature: 0.5
    max_tokens: 8192
    system_prompt: |
      You are a documentation agent specialized in creating clear,
      comprehensive documentation for code and systems.

  analysis_worker:
    provider: anthropic
    model: claude-sonnet-4-20250514
    temperature: 0.3
    max_tokens: 4096
    system_prompt: |
      You are an analysis agent specialized in understanding complex systems,
      identifying patterns, and providing actionable insights.

# Fallback configuration
fallback:
  # Enable automatic fallback to next provider if primary fails
  enabled: true
  # Order of providers to try on failure
  provider_order:
    - anthropic
    - openai
    - google
    - local
  # Maximum retry attempts per provider
  max_retries: 2
  # Backoff multiplier for retries (seconds)
  retry_backoff: 1.5

# Caching configuration
cache:
  enabled: true
  ttl_seconds: 3600
  max_entries: 1000
  # Cache key includes: provider, model, prompt hash
  include_temperature_in_key: true

# Logging and monitoring
monitoring:
  log_requests: true
  log_responses: false  # Set to true for debugging (may contain sensitive data)
  log_token_usage: true
  metrics_enabled: true
