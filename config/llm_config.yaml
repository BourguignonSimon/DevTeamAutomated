# ============================================
# Multi-Provider LLM Configuration
# ============================================
# This configuration file defines LLM providers, models, and settings
# for all agents in the DevTeamAutomated system.
#
# Supported providers:
#   - anthropic: Claude models (Claude 3.5, Claude 3, etc.)
#   - openai: OpenAI models (GPT-4, GPT-3.5, etc.)
#   - google: Google Gemini models (Gemini Pro, Gemini Ultra, etc.)
#   - local: Local LLM servers (Ollama, LocalAI, LMStudio, vLLM, etc.)

# ============================================
# Global Settings
# ============================================
global:
  # Default provider to use when not specified
  default_provider: anthropic

  # Fallback order when primary provider fails
  fallback_order:
    - anthropic
    - openai
    - google
    - local

  # Global timeout for LLM requests (seconds)
  timeout_seconds: 120

  # Maximum retries per provider before falling back
  max_retries: 3

  # Retry delay strategy (exponential backoff)
  retry_base_delay_seconds: 1
  retry_max_delay_seconds: 30

  # Enable caching of LLM responses
  cache_enabled: true
  cache_ttl_seconds: 3600

  # Enable request/response logging (set to false in production for privacy)
  logging_enabled: true
  log_level: INFO

# ============================================
# Provider Configurations
# ============================================
providers:
  # ------------------------------------------
  # Anthropic (Claude)
  # ------------------------------------------
  anthropic:
    enabled: true
    api_key: ${ANTHROPIC_API_KEY}
    base_url: https://api.anthropic.com
    api_version: "2023-06-01"

    # Default model for this provider
    default_model: claude-3-5-sonnet-20241022

    # Available models with their configurations
    models:
      claude-3-5-sonnet-20241022:
        max_tokens: 8192
        temperature: 0.7
        supports_vision: true
        supports_tools: true
        context_window: 200000
        cost_per_1k_input_tokens: 0.003
        cost_per_1k_output_tokens: 0.015

      claude-3-5-haiku-20241022:
        max_tokens: 8192
        temperature: 0.7
        supports_vision: true
        supports_tools: true
        context_window: 200000
        cost_per_1k_input_tokens: 0.001
        cost_per_1k_output_tokens: 0.005

      claude-3-opus-20240229:
        max_tokens: 4096
        temperature: 0.7
        supports_vision: true
        supports_tools: true
        context_window: 200000
        cost_per_1k_input_tokens: 0.015
        cost_per_1k_output_tokens: 0.075

      claude-3-sonnet-20240229:
        max_tokens: 4096
        temperature: 0.7
        supports_vision: true
        supports_tools: true
        context_window: 200000
        cost_per_1k_input_tokens: 0.003
        cost_per_1k_output_tokens: 0.015

      claude-3-haiku-20240307:
        max_tokens: 4096
        temperature: 0.7
        supports_vision: true
        supports_tools: true
        context_window: 200000
        cost_per_1k_input_tokens: 0.00025
        cost_per_1k_output_tokens: 0.00125

    # Provider-specific settings
    settings:
      timeout_seconds: 120
      max_retries: 3

  # ------------------------------------------
  # OpenAI
  # ------------------------------------------
  openai:
    enabled: true
    api_key: ${OPENAI_API_KEY}
    base_url: https://api.openai.com/v1
    organization_id: ${OPENAI_ORG_ID}

    # Default model for this provider
    default_model: gpt-4o

    # Available models with their configurations
    models:
      gpt-4o:
        max_tokens: 16384
        temperature: 0.7
        supports_vision: true
        supports_tools: true
        context_window: 128000
        cost_per_1k_input_tokens: 0.005
        cost_per_1k_output_tokens: 0.015

      gpt-4o-mini:
        max_tokens: 16384
        temperature: 0.7
        supports_vision: true
        supports_tools: true
        context_window: 128000
        cost_per_1k_input_tokens: 0.00015
        cost_per_1k_output_tokens: 0.0006

      gpt-4-turbo:
        max_tokens: 4096
        temperature: 0.7
        supports_vision: true
        supports_tools: true
        context_window: 128000
        cost_per_1k_input_tokens: 0.01
        cost_per_1k_output_tokens: 0.03

      gpt-4:
        max_tokens: 8192
        temperature: 0.7
        supports_vision: false
        supports_tools: true
        context_window: 8192
        cost_per_1k_input_tokens: 0.03
        cost_per_1k_output_tokens: 0.06

      gpt-3.5-turbo:
        max_tokens: 4096
        temperature: 0.7
        supports_vision: false
        supports_tools: true
        context_window: 16385
        cost_per_1k_input_tokens: 0.0005
        cost_per_1k_output_tokens: 0.0015

      o1-preview:
        max_tokens: 32768
        temperature: 1.0  # o1 models have fixed temperature
        supports_vision: false
        supports_tools: false
        context_window: 128000
        cost_per_1k_input_tokens: 0.015
        cost_per_1k_output_tokens: 0.06

      o1-mini:
        max_tokens: 65536
        temperature: 1.0
        supports_vision: false
        supports_tools: false
        context_window: 128000
        cost_per_1k_input_tokens: 0.003
        cost_per_1k_output_tokens: 0.012

    # Provider-specific settings
    settings:
      timeout_seconds: 120
      max_retries: 3

  # ------------------------------------------
  # Google (Gemini)
  # ------------------------------------------
  google:
    enabled: true
    api_key: ${GEMINI_API_KEY}
    base_url: https://generativelanguage.googleapis.com/v1beta

    # Default model for this provider
    default_model: gemini-1.5-pro

    # Available models with their configurations
    models:
      gemini-1.5-pro:
        max_tokens: 8192
        temperature: 0.7
        supports_vision: true
        supports_tools: true
        context_window: 2097152
        cost_per_1k_input_tokens: 0.00125
        cost_per_1k_output_tokens: 0.005

      gemini-1.5-flash:
        max_tokens: 8192
        temperature: 0.7
        supports_vision: true
        supports_tools: true
        context_window: 1048576
        cost_per_1k_input_tokens: 0.000075
        cost_per_1k_output_tokens: 0.0003

      gemini-1.5-flash-8b:
        max_tokens: 8192
        temperature: 0.7
        supports_vision: true
        supports_tools: true
        context_window: 1048576
        cost_per_1k_input_tokens: 0.0000375
        cost_per_1k_output_tokens: 0.00015

      gemini-2.0-flash-exp:
        max_tokens: 8192
        temperature: 0.7
        supports_vision: true
        supports_tools: true
        context_window: 1048576
        cost_per_1k_input_tokens: 0.0
        cost_per_1k_output_tokens: 0.0

      gemini-pro:
        max_tokens: 8192
        temperature: 0.7
        supports_vision: false
        supports_tools: true
        context_window: 32768
        cost_per_1k_input_tokens: 0.0005
        cost_per_1k_output_tokens: 0.0015

      gemini-pro-vision:
        max_tokens: 4096
        temperature: 0.7
        supports_vision: true
        supports_tools: false
        context_window: 16384
        cost_per_1k_input_tokens: 0.0005
        cost_per_1k_output_tokens: 0.0015

    # Provider-specific settings
    settings:
      timeout_seconds: 120
      max_retries: 3
      safety_settings:
        harassment: BLOCK_MEDIUM_AND_ABOVE
        hate_speech: BLOCK_MEDIUM_AND_ABOVE
        sexually_explicit: BLOCK_MEDIUM_AND_ABOVE
        dangerous_content: BLOCK_MEDIUM_AND_ABOVE

  # ------------------------------------------
  # Local LLM (Ollama, LocalAI, LMStudio, vLLM)
  # ------------------------------------------
  local:
    enabled: false  # Enable when using local LLM server

    # Local server configuration
    server_type: ollama  # Options: ollama, localai, lmstudio, vllm, openai_compatible
    base_url: ${LOCAL_LLM_URL:-http://localhost:11434}
    api_key: ${LOCAL_LLM_API_KEY}  # Optional, only if server requires auth

    # Default model for this provider
    default_model: llama3.1:8b

    # Available models - configure based on your local setup
    models:
      # Llama 3.1 models
      llama3.1:8b:
        max_tokens: 4096
        temperature: 0.7
        supports_vision: false
        supports_tools: true
        context_window: 131072

      llama3.1:70b:
        max_tokens: 4096
        temperature: 0.7
        supports_vision: false
        supports_tools: true
        context_window: 131072

      # Llama 3.2 models (with vision)
      llama3.2:11b-vision:
        max_tokens: 4096
        temperature: 0.7
        supports_vision: true
        supports_tools: true
        context_window: 131072

      # Mistral models
      mistral:7b:
        max_tokens: 4096
        temperature: 0.7
        supports_vision: false
        supports_tools: true
        context_window: 32768

      mixtral:8x7b:
        max_tokens: 4096
        temperature: 0.7
        supports_vision: false
        supports_tools: true
        context_window: 32768

      # Code-focused models
      codellama:13b:
        max_tokens: 4096
        temperature: 0.2
        supports_vision: false
        supports_tools: false
        context_window: 16384

      deepseek-coder:6.7b:
        max_tokens: 4096
        temperature: 0.2
        supports_vision: false
        supports_tools: false
        context_window: 16384

      # Qwen models
      qwen2.5:14b:
        max_tokens: 4096
        temperature: 0.7
        supports_vision: false
        supports_tools: true
        context_window: 131072

      # Phi models
      phi3:14b:
        max_tokens: 4096
        temperature: 0.7
        supports_vision: false
        supports_tools: true
        context_window: 131072

    # Provider-specific settings
    settings:
      timeout_seconds: 300  # Local models may be slower
      max_retries: 2
      # GPU settings (for vLLM/LocalAI)
      gpu_memory_utilization: 0.9
      tensor_parallel_size: 1

# ============================================
# Agent-Specific Configurations
# ============================================
# Override default provider/model settings for specific agents
agent_overrides:
  # Analysis team agents - use capable models for complex reasoning
  pattern_agent:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
    temperature: 0.3
    max_tokens: 4096

  extraction_agent:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
    temperature: 0.1
    max_tokens: 8192

  classifier_agent:
    provider: openai
    model: gpt-4o-mini
    temperature: 0.2
    max_tokens: 2048

  remediation_agent:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
    temperature: 0.5
    max_tokens: 4096

  report_agent:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
    temperature: 0.7
    max_tokens: 8192

  # Writing team - use models good at creative writing
  editor_agent:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
    temperature: 0.7
    max_tokens: 8192

  style_keeper_agent:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
    temperature: 0.5
    max_tokens: 4096

  # Email team
  parser_agent:
    provider: openai
    model: gpt-4o-mini
    temperature: 0.1
    max_tokens: 2048

  writer_agent:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
    temperature: 0.7
    max_tokens: 4096

  # Support team
  support_context_agent:
    provider: openai
    model: gpt-4o
    temperature: 0.3
    max_tokens: 4096

  support_answer_agent:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
    temperature: 0.7
    max_tokens: 4096

  support_analytics_agent:
    provider: google
    model: gemini-1.5-flash
    temperature: 0.2
    max_tokens: 4096

  # Admin team
  admin_classifier_agent:
    provider: openai
    model: gpt-4o-mini
    temperature: 0.1
    max_tokens: 1024

  admin_executor_agent:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
    temperature: 0.3
    max_tokens: 4096

  # Worker agents
  dev_worker:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
    temperature: 0.3
    max_tokens: 8192

  test_worker:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
    temperature: 0.2
    max_tokens: 4096

  requirements_manager:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
    temperature: 0.5
    max_tokens: 8192

# ============================================
# Task-Type Configurations
# ============================================
# Configure provider/model based on task type
task_type_overrides:
  code_generation:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
    temperature: 0.2
    max_tokens: 8192

  code_review:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
    temperature: 0.3
    max_tokens: 4096

  text_analysis:
    provider: openai
    model: gpt-4o
    temperature: 0.3
    max_tokens: 4096

  text_generation:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
    temperature: 0.7
    max_tokens: 8192

  summarization:
    provider: google
    model: gemini-1.5-flash
    temperature: 0.3
    max_tokens: 4096

  classification:
    provider: openai
    model: gpt-4o-mini
    temperature: 0.1
    max_tokens: 1024

  data_extraction:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
    temperature: 0.1
    max_tokens: 4096

  reasoning:
    provider: openai
    model: o1-mini
    temperature: 1.0
    max_tokens: 16384

# ============================================
# Rate Limiting Configuration
# ============================================
rate_limiting:
  enabled: true

  # Per-provider rate limits
  providers:
    anthropic:
      requests_per_minute: 60
      tokens_per_minute: 100000

    openai:
      requests_per_minute: 60
      tokens_per_minute: 150000

    google:
      requests_per_minute: 60
      tokens_per_minute: 120000

    local:
      requests_per_minute: 100
      tokens_per_minute: 1000000  # No real limit for local

# ============================================
# Cost Tracking Configuration
# ============================================
cost_tracking:
  enabled: true

  # Budget limits (in USD)
  daily_budget: 100.0
  monthly_budget: 2000.0

  # Alert thresholds (percentage of budget)
  alert_at_percentage: 80

  # Cost allocation
  track_by_agent: true
  track_by_task_type: true
  track_by_project: true
