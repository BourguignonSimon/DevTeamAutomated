# ============================================
# Redis Configuration
# ============================================
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_DB=0
# Redis password (optional, recommended for production)
# REDIS_PASSWORD=your_secure_password_here

# ============================================
# Namespace Configuration
# ============================================
# Default namespace for streams and keys (default: audit)
# Override to use the toolkit for different domains
NAMESPACE=audit

# Alternative preset namespaces:
# NAMESPACE=healthcare
# NAMESPACE=finserv
# NAMESPACE=manufacturing
# NAMESPACE=devsecops
# NAMESPACE=cicd

# Granular stream/key overrides (optional)
# If not set, these will be derived from NAMESPACE
# STREAM_NAME=audit:events
# DLQ_STREAM=audit:dlq
# KEY_PREFIX=audit
# TRACE_PREFIX=audit:trace
# METRICS_PREFIX=audit:metrics
# IDEMPOTENCE_PREFIX=audit:processed

# ============================================
# Consumer Configuration
# ============================================
# Consumer group name (default: {namespace}_stream_consumers)
# CONSUMER_GROUP=audit_stream_consumers
# Consumer instance name (default: consumer-1)
CONSUMER_NAME=consumer-1

# ============================================
# Stream Processing Settings
# ============================================
# Block duration for XREAD in milliseconds (default: 2000)
BLOCK_MS=2000
# Minimum idle time before reclaiming pending messages in milliseconds (default: 60000)
IDLE_RECLAIM_MS=60000
# Maximum number of pending messages to reclaim per iteration (default: 50)
PENDING_RECLAIM_COUNT=50

# ============================================
# Reliability Settings
# ============================================
# Maximum retry attempts before sending to DLQ (default: 5)
MAX_ATTEMPTS=5
# Time-to-live for idempotence tracking in seconds (default: 86400 = 24 hours)
DEDUPE_TTL_SECONDS=86400
IDEMPOTENCE_TTL_S=86400
# Lock TTL for distributed locks in seconds (default: 120)
LOCK_TTL_S=120

# ============================================
# Logging Configuration
# ============================================
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL (default: INFO)
LOG_LEVEL=INFO

# ============================================
# LLM Gateway Configuration
# ============================================
# LLM provider order (comma-separated, fallback order)
# Available providers: anthropic, openai, google, local
LLM_PROVIDER_ORDER=anthropic,openai,google,local

# LLM Test Mode (true = use mock responses, false = use real APIs)
# IMPORTANT: Set to "false" in production!
LLM_TEST_MODE=true

# LLM Gateway URL (for services that call it)
LLM_GATEWAY_URL=http://llm_gateway:8000

# LLM request timeout in seconds (default: 120)
LLM_TIMEOUT_S=120

# LLM max retries per provider (default: 2)
LLM_MAX_RETRIES=2

# LLM retry backoff multiplier (default: 1.5)
LLM_RETRY_BACKOFF=1.5

# LLM response cache TTL in seconds (default: 3600)
LLM_CACHE_TTL_S=3600

# LLM configuration file path (optional, for advanced configuration)
# LLM_CONFIG_PATH=/app/config/llm_providers.yaml

# ============================================
# LLM API Keys (Required when LLM_TEST_MODE=false)
# ============================================
# Anthropic Claude API Key (for Claude models)
# Get your key at: https://console.anthropic.com/
# ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# OpenAI GPT API Key (for GPT-4, GPT-4o, o1 models)
# Get your key at: https://platform.openai.com/
# OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Google Gemini API Key (for Gemini models)
# Get your key at: https://aistudio.google.com/
# GEMINI_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
# Alternative: GOOGLE_API_KEY

# ============================================
# LLM Default Models (Optional)
# ============================================
# Override default model for each provider
# ANTHROPIC_MODEL=claude-sonnet-4-20250514
# OPENAI_MODEL=gpt-4o
# GEMINI_MODEL=gemini-2.0-flash
# LOCAL_MODEL=llama3.2:latest

# ============================================
# Local LLM Configuration (Ollama/LM Studio)
# ============================================
# Local LLM API URL (default: Ollama OpenAI-compatible endpoint)
# For Ollama: http://localhost:11434/v1/chat/completions
# For LM Studio: http://localhost:1234/v1/chat/completions
LOCAL_LLM_URL=http://localhost:11434/v1/chat/completions

# ============================================
# Order Intake Agent Configuration
# ============================================
# Storage directory for uploaded files
STORAGE_DIR=/storage

# ============================================
# Service-Specific Overrides (Optional)
# ============================================
# Override settings for specific services if needed
# Format: PREFIX_SETTING_NAME

# Orchestrator specific
# ORCHESTRATOR_LOCK_TTL_S=300
# ORCHESTRATOR_IDEMPOTENCE_TTL_S=604800

# Worker specific timeouts, retries, etc.
# WORKER_TIMEOUT_S=300
